{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate 'str' and 'list' objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-292b8b1e577d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript_filename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranscript_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                         \u001b[0mmakeTfIdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# with open('businessContent.pickle', 'wb') as f:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-292b8b1e577d>\u001b[0m in \u001b[0;36mmakeTfIdf\u001b[0;34m(raw_html, index)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcontent_for_this_article\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbsoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'story-body-text story-content'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mcontent_for_this_article\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mfinal_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_for_this_article\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate 'str' and 'list' objects"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import sys\n",
    "import os\n",
    "import fnmatch\n",
    "import unicodedata\n",
    "import cPickle as pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "final_content = []\n",
    "token_dict = {}\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "def makeTfIdf(raw_html,index):\n",
    "\t# raw_html = raw_html.encode('utf-8').decode('ascii', 'ignore')\n",
    "\tbsoup = bs4.BeautifulSoup(raw_html)\n",
    "\tcontent_for_this_article = \"\"\n",
    "\tfor hit in bsoup.findAll(attrs={'class' : 'story-body-text story-content'}):\n",
    "\t\tcontent_for_this_article += ' '.join(tokenize(hit.text.rstrip()))\n",
    "\tfinal_content.append(content_for_this_article.encode('ascii', 'ignore'))\n",
    "\n",
    "for folder, subs, files in os.walk(\"business\"):\n",
    "\tfor index, transcript_filename in enumerate(fnmatch.filter(files, '*.html')):\n",
    "\t\twith open(os.path.join(folder, transcript_filename)) as f:\n",
    "\t\t\tmakeTfIdf(f.read(),index)\n",
    "\n",
    "# with open('businessContent.pickle', 'wb') as f:\n",
    "#     pickle.dump(final_content, f)\n",
    "\n",
    "corpus = final_content\n",
    "# tf = TfidfVectorizer(input='content', analyzer='word', min_df=1,max_features=5000, sublinear_tf=True,stop_words = 'english', norm = 'l2')\n",
    "# response =  tf.fit_transform(corpus)\n",
    "# feature_names = tf.get_feature_names()\n",
    "result = {}\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=1,  # min count for relevant vocabulary\n",
    "    max_features=4000,  # maximum number of features\n",
    "    stop_words = 'english',\n",
    "    strip_accents='unicode',  # replace all accented unicode char\n",
    "    # by their corresponding  ASCII char\n",
    "    analyzer='word',  # features made of words\n",
    "    token_pattern=r'\\w{4,}',  # tokenize only words of 4+ chars\n",
    "    ngram_range=(1, 1),  # features made of a single tokens\n",
    "    use_idf=True,  # enable inverse-document-frequency reweighting\n",
    "    smooth_idf=True,  # prevents zero division for unseen words\n",
    "    sublinear_tf=False)\n",
    "\n",
    "# vectorize and re-weight\n",
    "print(corpus)\n",
    "desc_vect = tfidf_vectorizer.fit_transform(corpus)\n",
    "d = dict(zip(tfidf_vectorizer.get_feature_names(), desc_vect.data))\n",
    "f = open('business.json', 'w')\n",
    "json.dump(d, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "# # print(feature_names)\n",
    "# # idf = tf.idf_\n",
    "# # print dict(zip(tf.get_feature_names(), idf))\n",
    "# indices = np.argsort(tf.idf_)[::-1]\n",
    "# top_n = 50\n",
    "# # top_features = [feature_names[i] for i in indices[:top_n]]\n",
    "# # print top_features\n",
    "# for col in sorted(response.nonzero()[1],reverse=True):\n",
    "# \tif(response[0,col]!=0.0):\n",
    "# \t\tresult[feature_names[col]] = response[0,col]\n",
    "# print result\n",
    "\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
